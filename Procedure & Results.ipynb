{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam/Ham Classifier<br>\n",
    "\n",
    "**Submitted By:** Divij Bhatia, divijbha@usc.edu<br>\n",
    "### Steps:<br>\n",
    ">**1.** Load the dataset<br>\n",
    ">**2.** Extract features and labels<br>\n",
    ">**3.** Shuffle the features and labels with respect to each other<br>\n",
    ">**4.** Define the value of k for k-fold cross validation<br>\n",
    ">**5.** For every fold from 1 to k do the following:<br>\n",
    ">>**5.1** Extract the training and testing dataset from the features and labels extracted in step 2<br>\n",
    ">>**5.2** Train the classifier on the training dataset to generate a model M<br>\n",
    ">>**5.3** Predict the labels of testing data using the model M<br>\n",
    ">>**5.4** Compare the predicted values with the actual labels to calculate the false positive rate, false negative rate and overall error rate.<br>\n",
    "\n",
    "### Properties of the Classifier:\n",
    "\n",
    ">**K:** 107 _(107-Fold Cross Validation)_<br>\n",
    ">**Classifier Used:** Multi Layer Perceptron<br>\n",
    ">**Layers:** 1 Input Layer : 57 Nodes, 1 Hidden Layer : 50 Nodes, 1 Output Layer : 2 Nodes<br>\n",
    ">**Activation Function:** ReLU<br>\n",
    ">**Solver:** Adam<br>\n",
    ">**Learning Rate**: Inverse Scaled<br>\n",
    ">**Initial Learning Rate:** 0.0001<br>\n",
    ">**Epochs:** 500 _(Maximum)_<br>\n",
    "\n",
    "\n",
    "### Results:<br>\n",
    "\n",
    "\n",
    "| K | False Positive Rate | False Negative Rate | Overall Error Rate |\n",
    "| --- | --- | --- | --- |\n",
    "|1 | 0.038461538 | 0.058823529 | 0.046511628| \n",
    "|2 | 0.04 | 0.111111111 | 0.069767442|\n",
    "|3 | 0.074074074 | 0.25 | 0.139534884|\n",
    "|4 | 0.1 | 0.130434783 | 0.11627907|\n",
    "|5 | 0.105263158 | 0.083333333 | 0.093023256|\n",
    "|6 | 0.0625 | 0 | 0.046511628|\n",
    "|7 | 0 | 0 | 0|\n",
    "|8 | 0.043478261 | 0.15 | 0.093023256|\n",
    "|9 | 0.045454545 | 0.047619048 | 0.046511628|\n",
    "|10 | 0.076923077 | 0.117647059 | 0.093023256\n",
    "|11 | 0 | 0.1 | 0.046511628\n",
    "|12 | 0.076923077 | 0.176470588 | 0.11627907|\n",
    "|13 | 0 | 0.058823529 | 0.023255814|\n",
    "|14 | 0.034482759 | 0.142857143 | 0.069767442|\n",
    "|15 | 0.034482759 | 0.142857143 | 0.069767442|\n",
    "|16 | 0 | 0.0625 | 0.023255814|\n",
    "|17 | 0 | 0.277777778 | 0.11627907|\n",
    "|18 | 0.04 | 0.166666667 | 0.093023256|\n",
    "|19 | 0.086956522 | 0.05 | 0.069767442|\n",
    "|20 | 0.043478261 | 0.05 | 0.046511628|\n",
    "|21 | 0.142857143 | 0.066666667 | 0.11627907|\n",
    "|22 | 0.043478261 | 0.15 | 0.093023256|\n",
    "|23 | 0.076923077 | 0.117647059 | 0.093023256|\n",
    "|24 | 0.064516129 | 0.25 | 0.11627907|\n",
    "|25 | 0.037037037 | 0.0625 | 0.046511628|\n",
    "|26 | 0.032258065 | 0.25 | 0.093023256|\n",
    "|27 | 0 | 0.111111111 | 0.046511628|\n",
    "|28 | 0.034482759 | 0 | 0.023255814|\n",
    "|29 | 0.035714286 | 0.066666667 | 0.046511628|\n",
    "|30 | 0.08 | 0.277777778 | 0.162790698|\n",
    "|31 | 0 | 0.05 | 0.023255814|\n",
    "|32 | 0.136363636 | 0 | 0.069767442|\n",
    "|33 | 0.037037037 | 0.0625 | 0.046511628|\n",
    "|34 | 0.086956522 | 0 | 0.046511628|\n",
    "|35 | 0 | 0 | 0|\n",
    "|36 | 0 | 0.117647059 | 0.046511628|\n",
    "|37 | 0.074074074 | 0 | 0.046511628|\n",
    "|38 | 0 | 0 | 0|\n",
    "|39 | 0.035714286 | 0 | 0.023255814|\n",
    "|40 | 0 | 0.090909091 | 0.046511628|\n",
    "|41 | 0 | 0.133333333 | 0.046511628|\n",
    "|42 | 0.037037037 | 0.0625 | 0.046511628|\n",
    "|43 | 0.033333333 | 0.307692308 | 0.11627907|\n",
    "|44 | 0.12 | 0 | 0.069767442|\n",
    "|45 | 0.076923077 | 0.235294118 | 0.139534884|\n",
    "|46 | 0.074074074 | 0.0625 | 0.069767442|\n",
    "|47 | 0 | 0.045454545 | 0.023255814|\n",
    "|48 | 0.045454545 | 0.095238095 | 0.069767442|\n",
    "|49 | 0.107142857 | 0.066666667 | 0.093023256|\n",
    "|50 | 0.2 | 0 | 0.093023256|\n",
    "|51 | 0.045454545 | 0.047619048 | 0.046511628|\n",
    "|52 | 0.076923077 | 0.058823529 | 0.069767442|\n",
    "|53 | 0.034482759 | 0 | 0.023255814|\n",
    "|54 | 0.04 | 0.055555556 | 0.046511628|\n",
    "|55 | 0.045454545 | 0.142857143 | 0.093023256|\n",
    "|56 | 0 | 0 | 0|\n",
    "|57 | 0 | 0.166666667 | 0.069767442|\n",
    "|58 | 0 | 0.111111111 | 0.046511628|\n",
    "|59 | 0.083333333 | 0.210526316 | 0.139534884|\n",
    "|60 | 0 | 0.0625 | 0.023255814|\n",
    "|61 | 0.035714286 | 0.066666667 | 0.046511628|\n",
    "|62 | 0.076923077 | 0.176470588 | 0.11627907|\n",
    "|63 | 0.047619048 | 0.045454545 | 0.046511628|\n",
    "|64 | 0 | 0 | 0|\n",
    "|65 | 0 | 0.25 | 0.093023256|\n",
    "|66 | 0 | 0.1 | 0.023255814|\n",
    "|67 | 0.037037037 | 0.0625 | 0.046511628|\n",
    "|68 | 0.035714286 | 0.2 | 0.093023256|\n",
    "|69 | 0.04 | 0.055555556 | 0.046511628|\n",
    "|70 | 0 | 0.0625 | 0.023255814|\n",
    "|71 | 0.08 | 0.055555556 | 0.069767442|\n",
    "|72 | 0.066666667 | 0.153846154 | 0.093023256|\n",
    "|73 | 0.068965517 | 0.071428571 | 0.069767442|\n",
    "|74 | 0 | 0.058823529 | 0.023255814|\n",
    "|75 | 0.076923077 | 0.058823529 | 0.069767442|\n",
    "|76 | 0.107142857 | 0.133333333 | 0.11627907|\n",
    "|77 | 0.038461538 | 0.058823529 | 0.046511628|\n",
    "|78 | 0.142857143 | 0 | 0.093023256|\n",
    "|79 | 0.076923077 | 0.117647059 | 0.093023256|\n",
    "|80 | 0 | 0.045454545 | 0.023255814|\n",
    "|81 | 0.041666667 | 0.052631579 | 0.046511628|\n",
    "|82 | 0 | 0.214285714 | 0.069767442|\n",
    "|83 | 0.066666667 | 0.076923077 | 0.069767442|\n",
    "|84 | 0 | 0.105263158 | 0.046511628|\n",
    "|85 | 0.083333333 | 0.157894737 | 0.11627907|\n",
    "|86 | 0.038461538 | 0.058823529 | 0.046511628|\n",
    "|87 | 0.111111111 | 0.0625 | 0.093023256|\n",
    "|88 | 0 | 0 | 0|\n",
    "|89 | 0.0625 | 0 | 0.046511628|\n",
    "|90 | 0.035714286 | 0.133333333 | 0.069767442|\n",
    "|91 | 0.037037037 | 0 | 0.023255814|\n",
    "|92 | 0.09375 | 0 | 0.069767442|\n",
    "|93 | 0.035714286 | 0.2 | 0.093023256|\n",
    "|94 | 0.034482759 | 0.071428571 | 0.046511628|\n",
    "|95 | 0 | 0 | 0|\n",
    "|96 | 0 | 0.166666667 | 0.046511628|\n",
    "|97 | 0.052631579 | 0.041666667 | 0.046511628|\n",
    "|98 | 0.08 | 0 | 0.046511628|\n",
    "|99 | 0.068965517 | 0.071428571 | 0.069767442|\n",
    "|100 | 0.034482759 | 0 | 0.023255814|\n",
    "|101 | 0.074074074 | 0.125 | 0.093023256|\n",
    "|102 | 0.047619048 | 0.045454545 | 0.046511628|\n",
    "|103 | 0.037037037 | 0 | 0.023255814|\n",
    "|104 | 0.037037037 | 0.0625 | 0.046511628|\n",
    "|105 | 0.115384615 | 0.117647059 | 0.11627907|\n",
    "|106 | 0.068965517 | 0.142857143 | 0.093023256|\n",
    "|107 | 0 | 0.0625 | 0.023255814|\n",
    "|**Average** | **0.046982112** | **0.088377319** | **0.062377744**|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code:\n",
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read & Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read datafile\n",
    "data = np.genfromtxt('spambase.data', delimiter=',',dtype='float')\n",
    "\n",
    "#extract features\n",
    "features=data[:,:-1]\n",
    "\n",
    "#extract labels i.e. last column in the data file\n",
    "labels = data[:,-1]\n",
    "\n",
    "# shuffle the data\n",
    "features,labels = shuffle(features,labels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters for k-fold cross validation\n",
    "n=len(data) #size of data\n",
    "k=107 \n",
    "len_of_fold=int(n/k) #size of one of the k segments or the size of test data\n",
    "l_index=0 #initial starting index of test data\n",
    "h_index=len_of_fold #initial ending index of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test using K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    #extract test dataset from all the features\n",
    "    features_test=features[l_index:h_index]\n",
    "    \n",
    "    #extract actual labels of test dataset\n",
    "    labels_test=labels[l_index:h_index]\n",
    "    \n",
    "    #count number of spam examples in test set\n",
    "    spam_test_ex=np.count_nonzero(labels_test)\n",
    "    \n",
    "    #count number of non-spam examples in test set\n",
    "    #count_of_non_spam_examples_in_test_dataset = size_of_test_dataset - count_of_spam_examples_in_test_data \n",
    "    non_spam_test_ex=len_of_fold-spam_test_ex\n",
    "    \n",
    "    #extracting training dataset features\n",
    "    features_train=np.concatenate((features[0:l_index],features[h_index:]),axis=0)\n",
    "    \n",
    "    #extracting training dataset labels\n",
    "    labels_train=np.concatenate((labels[0:l_index],labels[h_index:]),axis=0)\n",
    "    \n",
    "    #Instantiating Multi-Layered Perceptron (MLP)\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', learning_rate='invscaling', learning_rate_init=0.0001, power_t=0.5, max_iter=500)\n",
    "    \n",
    "    #Training MLP\n",
    "    classifier.fit(features_train,labels_train)\n",
    "    \n",
    "    #Testing data using the trained model\n",
    "    prediction=classifier.predict(features_test)\n",
    "    \n",
    "    #counting misclassified examples\n",
    "    fp=0 #false_positive_counter initialized to 0\n",
    "    fn=0 #false_negative_counter initialized to 0\n",
    "    \n",
    "    for j in range(len_of_fold):\n",
    "        #false_positive_counter is incremented if a non_spam email is classified as spam\n",
    "        if prediction[j]==1 and prediction[j]!=labels_test[j]:\n",
    "            fp+=1\n",
    "        #false_negative_counter is incremented if a spam email is classified as non_spam\n",
    "        elif prediction[j]==0 and prediction[j]!=labels_test[j]:\n",
    "            fn+=1\n",
    "    \n",
    "    #printing K, false_positive_rate, false_negative_rate, overall_error_rate\n",
    "    print(i+1,\",\",fp/non_spam_test_ex,\",\",fn/spam_test_ex,\",\",(fp+fn)/len_of_fold)\n",
    "    \n",
    "    #incrementing the indexes of the test dataset\n",
    "    l_index+=len_of_fold\n",
    "    h_index+=len_of_fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
